import asyncio
import logging
import os
import re
from threading import Thread
from flask import Flask

from aiogram import Bot, Dispatcher, types, Router, F
from aiogram.enums import ParseMode
from aiogram.exceptions import TelegramBadRequest
from aiogram.filters import Command, StateFilter
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.types import (InlineKeyboardButton, InlineKeyboardMarkup,
                           KeyboardButton, ReplyKeyboardMarkup,
                           ReplyKeyboardRemove, FSInputFile)
from dotenv import load_dotenv
import google.generativeai as genai
from gtts import gTTS
from pydub import AudioSegment
import torch
import whisper

# ------------------ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ------------------
load_dotenv()
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not os.path.exists("temp"):
    os.makedirs("temp")

logging.basicConfig(level=logging.INFO)

# ------------------ –î–ï–§–û–õ–¢–ù–ê–Ø –°–ò–°–¢–ï–ú–ù–ê–Ø –ò–ù–°–¢–†–£–ö–¶–ò–Ø ------------------
DEFAULT_SYSTEM_PROMPT = """
–¢—ã ‚Äî –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ Telegram. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
–ê–ë–°–û–õ–Æ–¢–ù–û –í–°–ï–ì–î–ê, –±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏–π, —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–π —Å–≤–æ–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É—è —Å–∏–Ω—Ç–∞–∫—Å–∏—Å MarkdownV2 –¥–ª—è Telegram.

**–ü—Ä–∞–≤–∏–ª–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**
- –ñ–∏—Ä–Ω—ã–π: **—Ç–µ–∫—Å—Ç**
- –ö—É—Ä—Å–∏–≤: *—Ç–µ–∫—Å—Ç*
- –ü–æ–¥—á–µ—Ä–∫–Ω—É—Ç—ã–π: __—Ç–µ–∫—Å—Ç__
- –ó–∞—á–µ—Ä–∫–Ω—É—Ç—ã–π: ~—Ç–µ–∫—Å—Ç~
- –ú–æ–Ω–æ—à–∏—Ä–∏–Ω–Ω—ã–π –∫–æ–¥ (–∏–Ω–ª–∞–π–Ω): `—Ç–µ–∫—Å—Ç`
- –ë–ª–æ–∫ —Å –∫–æ–¥–æ–º: ```python\n–∫–æ–¥\n```
- –°—Å—ã–ª–∫–∏: [—Ç–µ–∫—Å—Ç](URL)

**–ó–ê–ü–†–ï–©–ï–ù–û:**
- –ö–∞—Ç–µ–≥–æ—Ä–∏—á–µ—Å–∫–∏ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Å –ø–æ–º–æ—â—å—é —Å–∏–º–≤–æ–ª–æ–≤ #.

**–û–ß–ï–ù–¨ –í–ê–ñ–ù–û (–≠–ö–†–ê–ù–ò–†–û–í–ê–ù–ò–ï):**
- –í—Å–µ–≥–¥–∞ —ç–∫—Ä–∞–Ω–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –¥–æ–±–∞–≤–ª—è—è –ø–µ—Ä–µ–¥ –Ω–∏–º–∏ –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª—ç—à (\\\\): `._*[]()~>#+-=|{}!`
"""

# ------------------ –ö–ª–∞–≤–∏–∞—Ç—É—Ä—ã ------------------
button_new_request = KeyboardButton(text="‚úÖ –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
keyboard = ReplyKeyboardMarkup(keyboard=[[button_new_request]], resize_keyboard=True, input_field_placeholder="–ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å...")
button_idea = InlineKeyboardButton(text="üí° –ò–¥–µ—è –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞", callback_data="idea")
button_poem = InlineKeyboardButton(text="‚úçÔ∏è –ù–∞–ø–∏—à–∏ —Å—Ç–∏—Ö", callback_data="poem")
button_story = InlineKeyboardButton(text="üìù –ù–∞–ø–∏—Å–∞—Ç—å —Ä–∞—Å—Å–∫–∞–∑", callback_data="story")
button_travel = InlineKeyboardButton(text="‚úàÔ∏è –°–ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ", callback_data="travel")
button_recipe = InlineKeyboardButton(text="üç≥ –†–µ—Ü–µ–ø—Ç –ø–æ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç–∞–º", callback_data="recipe")
inline_keyboard = InlineKeyboardMarkup(inline_keyboard=[
    [button_idea, button_poem],
    [button_story, button_travel],
    [button_recipe]
])

# ------------------ –ö–ª–∏–µ–Ω—Ç—ã AI ------------------
class GeminiClient:
    def __init__(self, api_key: str):
        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel("gemini-1.5-flash")
        except Exception as e:
            logging.critical(f"–û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Gemini: {e}")
            self.model = None

    async def generate_text(self, prompt: str) -> str:
        if not self.model: return "–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å Gemini –Ω–µ –±—ã–ª–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞."
        try:
            response = await asyncio.to_thread(self.model.generate_content, prompt)
            return response.text
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ Gemini: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

class LocalSpeechClient:
    def __init__(self, model_name="base"):
        self.model = None
        try:
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logging.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Whisper '{model_name}' –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ '{device}'...")
            self.model = whisper.load_model(model_name, device=device)
            logging.info("–ú–æ–¥–µ–ª—å Whisper —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")
        except Exception as e:
            logging.critical(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Whisper: {e}\n–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —É –≤–∞—Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã 'torch' –∏ 'openai-whisper'.")

    async def audio_to_text(self, audio_filepath: str) -> str | None:
        if not self.model: return None
        try:
            use_fp16 = self.model.device.type == 'cuda'
            result = await asyncio.to_thread(self.model.transcribe, audio_filepath, fp16=use_fp16, language='ru')
            logging.info(f"–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {result['text']}")
            return result['text']
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∞—É–¥–∏–æ: {e}")
            return None

class TTSClient:
    async def text_to_audio(self, text: str, output_filepath: str) -> bool:
        try:
            clean_text = re.sub(r'[*_`~[\]()\\#+-.!{}]', '', text).replace("```", " ").replace("`", " ")
            if not clean_text.strip(): return False
            def sync_tts():
                gTTS(clean_text, lang='ru').save(output_filepath)
                return True
            return await asyncio.to_thread(sync_tts)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∏–Ω—Ç–µ–∑–µ —Ä–µ—á–∏ (gTTS): {e}")
            return False

# ------------------ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ------------------
if not TELEGRAM_TOKEN or not GEMINI_API_KEY:
    raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è TELEGRAM_TOKEN –∏–ª–∏ GEMINI_API_KEY")

gemini_client = GeminiClient(GEMINI_API_KEY)
local_speech_client = LocalSpeechClient(model_name="base")
tts_client = TTSClient()
storage = MemoryStorage()
bot = Bot(token=TELEGRAM_TOKEN)
dp = Dispatcher(storage=storage)
router = Router()
dp.include_router(router)

# ------------------ –°–æ—Å—Ç–æ—è–Ω–∏—è FSM ------------------
class Form(StatesGroup):
    waiting_for_startup_area = State()
    waiting_for_poem_topic = State()
    waiting_for_system_prompt = State()
    waiting_for_story_prompt = State()
    waiting_for_travel_details = State()
    waiting_for_ingredients = State()

# ------------------ –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ------------------
def sanitize_markdown_v2(text: str) -> str:
    escape_chars = r'._*[]()~>#+-=|{}!'
    for char in escape_chars:
        pattern = r'(?<!\\)' + re.escape(char)
        replacement = r'\\' + char
        text = re.sub(pattern, replacement, text)
    return text

async def build_gemini_prompt(history: list, new_prompt: str, state: FSMContext) -> str:
    data = await state.get_data()
    user_system_prompt = data.get("system_prompt")
    
    full_system_prompt_parts = [DEFAULT_SYSTEM_PROMPT]
    if user_system_prompt:
        full_system_prompt_parts.append(f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_system_prompt}")
    
    system_part = "\n\n---\n\n".join(full_system_prompt_parts)

    history_part = ""
    if history:
        formatted_history = "\n".join(history)
        history_part = f"""
---
**–ö–û–ù–¢–ï–ö–°–¢ –ü–†–ï–î–´–î–£–©–ï–ì–û –î–ò–ê–õ–û–ì–ê (–¥–ª—è —Å–ø—Ä–∞–≤–∫–∏):**
{formatted_history}
---
"""
    new_prompt_part = f"""
**–ê–ö–¢–£–ê–õ–¨–ù–´–ô –ó–ê–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø (–æ—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –Ω–µ–≥–æ):**
{new_prompt}
"""
    return f"{system_part}{history_part}{new_prompt_part}"

async def send_formatted_answer(message: types.Message, text: str):
    if not text:
        await message.answer("–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ —Å–º–æ–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.", reply_markup=keyboard)
        return
    sanitized_text = sanitize_markdown_v2(text)
    try:
        for i in range(0, len(sanitized_text), 4096):
            chunk = sanitized_text[i:i + 4096]
            await message.answer(chunk, parse_mode=ParseMode.MARKDOWN_V2, reply_markup=keyboard)
    except TelegramBadRequest:
        logging.warning("–û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è MarkdownV2. –û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.")
        for i in range(0, len(text), 4096):
            chunk = text[i:i + 4096]
            await message.answer(chunk, reply_markup=keyboard)
    except Exception as e:
        logging.error(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
        await message.answer("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ—Ç–≤–µ—Ç–∞.", reply_markup=keyboard)

# ------------------ –û–ë–†–ê–ë–û–¢–ß–ò–ö–ò ------------------
@router.message(Command(commands=["start", "help", "setting"]))
async def handle_commands(message: types.Message, state: FSMContext):
    command = message.text.split()[0]
    if command == "/start":
        await state.clear()
        await message.answer("–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ Gemini. –ó–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å —Ç–µ–∫—Å—Ç–æ–º, –≥–æ–ª–æ—Å–æ–º –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ –æ–ø—Ü–∏—é:", reply_markup=inline_keyboard)
    elif command == "/help":
        help_text = "*–°–ø—Ä–∞–≤–∫–∞ –ø–æ –±–æ—Ç—É*\n\n`/start` \\- –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –±–æ—Ç–∞ –∏ —Å–±—Ä–æ—Å–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç\\.\n`/help` \\- –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ\\.\n`/setting` \\- –ó–∞–¥–∞—Ç—å *–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é* —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é \\(—Ä–æ–ª—å\\) –¥–ª—è –ò–ò\\.\n\n–í—ã –º–æ–∂–µ—Ç–µ –æ–±—â–∞—Ç—å—Å—è —Å–æ –º–Ω–æ–π –∫–∞–∫ —Ç–µ–∫—Å—Ç–æ–º, —Ç–∞–∫ –∏ *–≥–æ–ª–æ—Å–æ–≤—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏*\\."
        await message.answer(help_text, parse_mode=ParseMode.MARKDOWN_V2)
    elif command == "/setting":
        await state.set_state(Form.waiting_for_system_prompt)
        await message.answer("–í–≤–µ–¥–∏—Ç–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é.", reply_markup=ReplyKeyboardRemove())

@router.message(lambda message: message.text == "‚úÖ –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å")
async def new_request(message: types.Message, state: FSMContext):
    await state.clear()
    await message.answer("–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–±—Ä–æ—à–µ–Ω. –ú–æ–∂–µ—Ç–µ –∑–∞–¥–∞—Ç—å –Ω–æ–≤—ã–π –≤–æ–ø—Ä–æ—Å.", reply_markup=keyboard)

@router.message(StateFilter(Form.waiting_for_system_prompt))
async def process_system_prompt(message: types.Message, state: FSMContext):
    await state.update_data(system_prompt=message.text)
    await state.set_state(None)
    await message.answer("‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.", reply_markup=keyboard)

@router.message(StateFilter(Form.waiting_for_startup_area))
async def process_startup_area(message: types.Message, state: FSMContext):
    thinking_message = await message.answer("üí° –ì–µ–Ω–µ—Ä–∏—Ä—É—é –∏–¥–µ—é –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    await state.set_state(None)
    user_prompt = f"–ü—Ä–∏–¥—É–º–∞–π –∏ –ø–æ–¥—Ä–æ–±–Ω–æ –æ–ø–∏—à–∏ –∏–¥–µ—é –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞ –≤ —Å—Ñ–µ—Ä–µ: {message.text}"
    full_prompt = await build_gemini_prompt([], user_prompt, state)
    gpt_response = await gemini_client.generate_text(full_prompt)
    await thinking_message.delete()
    await send_formatted_answer(message, gpt_response)

@router.message(StateFilter(Form.waiting_for_poem_topic))
async def process_poem_topic(message: types.Message, state: FSMContext):
    thinking_message = await message.answer("‚úçÔ∏è –ü–∏—à—É —Å—Ç–∏—Ö\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    await state.set_state(None)
    user_prompt = f"–ù–∞–ø–∏—à–∏ –∫—Ä–∞—Å–∏–≤—ã–π —Å—Ç–∏—Ö –Ω–∞ —Ç–µ–º—É: {message.text}"
    full_prompt = await build_gemini_prompt([], user_prompt, state)
    gpt_response = await gemini_client.generate_text(full_prompt)
    await thinking_message.delete()
    await send_formatted_answer(message, gpt_response)

@router.message(StateFilter(Form.waiting_for_story_prompt))
async def process_story_prompt(message: types.Message, state: FSMContext):
    thinking_message = await message.answer("üìù –°–æ—á–∏–Ω—è—é —Ä–∞—Å—Å–∫–∞–∑\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    await state.set_state(None)
    user_prompt = f"–ù–∞–ø–∏—à–∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –∫–æ—Ä–æ—Ç–∫–∏–π —Ä–∞—Å—Å–∫–∞–∑ –Ω–∞ —Ç–µ–º—É: {message.text}"
    full_prompt = await build_gemini_prompt([], user_prompt, state)
    gpt_response = await gemini_client.generate_text(full_prompt)
    await thinking_message.delete()
    await send_formatted_answer(message, gpt_response)

@router.message(StateFilter(Form.waiting_for_travel_details))
async def process_travel_details(message: types.Message, state: FSMContext):
    thinking_message = await message.answer("‚úàÔ∏è –ü–ª–∞–Ω–∏—Ä—É—é –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    await state.set_state(None)
    user_prompt = f"–°–æ—Å—Ç–∞–≤—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –ø–ª–∞–Ω –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏—è. –î–µ—Ç–∞–ª–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {message.text}."
    full_prompt = await build_gemini_prompt([], user_prompt, state)
    gpt_response = await gemini_client.generate_text(full_prompt)
    await thinking_message.delete()
    await send_formatted_answer(message, gpt_response)

@router.message(StateFilter(Form.waiting_for_ingredients))
async def process_ingredients(message: types.Message, state: FSMContext):
    thinking_message = await message.answer("üç≥ –ò—â—É —Ä–µ—Ü–µ–ø—Ç\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    await state.set_state(None)
    user_prompt = f"–ü—Ä–∏–¥—É–º–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π —Ä–µ—Ü–µ–ø—Ç, –∏—Å–ø–æ–ª—å–∑—É—è —Å–ª–µ–¥—É—é—â–∏–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã: {message.text}."
    full_prompt = await build_gemini_prompt([], user_prompt, state)
    gpt_response = await gemini_client.generate_text(full_prompt)
    await thinking_message.delete()
    await send_formatted_answer(message, gpt_response)

@router.callback_query(lambda c: c.data in ["idea", "poem", "story", "travel", "recipe"])
async def process_callbacks(callback_query: types.CallbackQuery, state: FSMContext):
    await bot.answer_callback_query(callback_query.id)
    actions = {
        "idea": ("–í –∫–∞–∫–æ–π —Å—Ñ–µ—Ä–µ –≤—ã —Ö–æ—Ç–∏—Ç–µ –∏–¥–µ—é –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞?", Form.waiting_for_startup_area),
        "poem": ("–ù–∞ –∫–∞–∫—É—é —Ç–µ–º—É –Ω–∞–ø–∏—Å–∞—Ç—å —Å—Ç–∏—Ö?", Form.waiting_for_poem_topic),
        "story": ("–û —á–µ–º –Ω–∞–ø–∏—Å–∞—Ç—å —Ä–∞—Å—Å–∫–∞–∑?", Form.waiting_for_story_prompt),
        "travel": ("–ö—É–¥–∞ –∏ –Ω–∞ —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–µ—Ö–∞—Ç—å?", Form.waiting_for_travel_details),
        "recipe": ("–ö–∞–∫–∏–µ –∏–Ω–≥—Ä–µ–¥–∏–µ–Ω—Ç—ã —É –≤–∞—Å –µ—Å—Ç—å? (–ø–µ—Ä–µ—á–∏—Å–ª–∏—Ç–µ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)", Form.waiting_for_ingredients)
    }
    text, new_state = actions[callback_query.data]
    await bot.send_message(callback_query.from_user.id, text, reply_markup=ReplyKeyboardRemove())
    await state.set_state(new_state)

@router.message(F.voice)
async def handle_voice(message: types.Message, state: FSMContext):
    processing_message = await message.answer("Ô∏èÔ∏èüéôÔ∏è –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    voice_file_id = message.voice.file_id
    oga_filepath = os.path.join("temp", f"{voice_file_id}.oga")
    mp3_filepath = os.path.join("temp", f"{voice_file_id}.mp3")
    response_audio_path = os.path.join("temp", f"response_{voice_file_id}.mp3")

    try:
        file_info = await bot.get_file(voice_file_id)
        await bot.download_file(file_info.file_path, destination=oga_filepath)
        audio = await asyncio.to_thread(AudioSegment.from_file, oga_filepath, format="ogg")
        await asyncio.to_thread(audio.export, mp3_filepath, format="mp3")

        user_text = await local_speech_client.audio_to_text(mp3_filepath)
        if not user_text:
            await processing_message.edit_text("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –≤ —Å–æ–æ–±—â–µ–Ω–∏–∏\\.", parse_mode=ParseMode.MARKDOWN_V2)
            return

        sanitized_user_text = sanitize_markdown_v2(user_text)
        await processing_message.edit_text(f"–í—ã —Å–∫–∞–∑–∞–ª–∏: *¬´{sanitized_user_text}¬ª*\n\nüß† –î—É–º–∞—é –Ω–∞–¥ –æ—Ç–≤–µ—Ç–æ–º\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)

        data = await state.get_data()
        history = data.get('chat_history', [])

        full_prompt = await build_gemini_prompt(history, user_text, state)
        raw_response = await gemini_client.generate_text(full_prompt)
        clean_response = raw_response.strip()
        if clean_response.lower().startswith("–±–æ—Ç:"):
            clean_response = clean_response[4:].lstrip()

        history.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_text}")
        history.append(f"–ë–æ—Ç: {clean_response}")
        await state.update_data(chat_history=history[-10:])

        await processing_message.delete()
        await send_formatted_answer(message, clean_response)

        tts_success = await tts_client.text_to_audio(clean_response, response_audio_path)
        if tts_success and os.path.exists(response_audio_path):
            await message.answer_voice(FSInputFile(response_audio_path))

    except Exception as e:
        logging.error(f"–ü–æ–ª–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ handle_voice: {e}")
        try: await processing_message.edit_text("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ\\.", parse_mode=ParseMode.MARKDOWN_V2)
        except TelegramBadRequest: pass
    finally:
        for f in [oga_filepath, mp3_filepath, response_audio_path]:
            if os.path.exists(f): os.remove(f)

@router.message(F.text)
async def chat_with_gpt(message: types.Message, state: FSMContext):
    thinking_message = await message.answer("üß† –î—É–º–∞—é –Ω–∞–¥ –≤–∞—à–∏–º –∑–∞–ø—Ä–æ—Å–æ–º\\.\\.\\.", parse_mode=ParseMode.MARKDOWN_V2)
    try:
        data = await state.get_data()
        history = data.get('chat_history', [])
        
        full_prompt = await build_gemini_prompt(history, message.text, state)
        
        raw_response = await gemini_client.generate_text(full_prompt)
        clean_response = raw_response.strip()
        if clean_response.lower().startswith("–±–æ—Ç:"):
            clean_response = clean_response[4:].lstrip()
        
        history.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {message.text}")
        history.append(f"–ë–æ—Ç: {clean_response}")
        await state.update_data(chat_history=history[-10:])

        await thinking_message.delete()
        await send_formatted_answer(message, clean_response)

    except Exception as e:
        logging.error(f"–ü–æ–ª–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ chat_with_gpt: {e}")
        try: await thinking_message.edit_text("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ\\.", parse_mode=ParseMode.MARKDOWN_V2)
        except TelegramBadRequest: pass

# ------------------ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ –∏ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞ ------------------
app = Flask(__name__)

@app.route('/')
def index():
    return "I'm alive!"

def run_web_server():
    port = int(os.environ.get('PORT', 10000))
    app.run(host='0.0.0.0', port=port)

async def start_bot():
    commands = [
        types.BotCommand(command="/start", description="–ù–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É"),
        types.BotCommand(command="/help", description="–°–ø—Ä–∞–≤–∫–∞"),
        types.BotCommand(command="/setting", description="–ó–∞–¥–∞—Ç—å –¥–æ–ø. –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é")
    ]
    await bot.set_my_commands(commands)
    await dp.start_polling(bot)

if __name__ == '__main__':
    web_thread = Thread(target=run_web_server)
    web_thread.start()
    
    try:
        logging.info("Starting bot polling...")
        asyncio.run(start_bot())
    except (KeyboardInterrupt, SystemExit):
        logging.info("Bot stopped.")